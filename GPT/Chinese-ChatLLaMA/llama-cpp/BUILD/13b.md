

### Check out source code

> Transformer Model

```shell
git lfs install
git clone https://huggingface.co/elinas/llama-13b-hf-transformers-4.29
```



> Lora Model

Take model `13b` as an example, please click [here](https://huggingface.co/ziqingyang)  for detailed model links.

```shell
git lfs install
git clone https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b
```



> project

```shell
git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca
git clone https://github.com/ggerganov/llama.cpp
```





```shell
cd Chinese-LLaMA-Alpaca

python ./scripts/merge_llama_with_chinese_lora_low_mem.py \
    --base_model '../llama-13b-hf-transformers-4.29' \
    --lora_model '../chinese-alpaca-lora-13b' \
    --output_type pth \
    --output_dir alpaca-combined
```

And the log like as below:

```ini
Base model: ../llama-13b-hf-transformers-4.29
LoRA model(s) ['../chinese-alpaca-lora-13b']:
Loading ../chinese-alpaca-lora-13b
Loading ckpt pytorch_model-00001-of-00003.bin
Merging...
Converting to pth format...
Saving shard 1 of 2 into alpaca-combined/L1-consolidated.00.pth
Saving shard 2 of 2 into alpaca-combined/L1-consolidated.01.pth
Loading ckpt pytorch_model-00002-of-00003.bin
Merging...
Converting to pth format...
Saving shard 1 of 2 into alpaca-combined/L2-consolidated.00.pth
Saving shard 2 of 2 into alpaca-combined/L2-consolidated.01.pth
Loading ckpt pytorch_model-00003-of-00003.bin
Merging...
Converting to pth format...
Saving shard 1 of 2 into alpaca-combined/L3-consolidated.00.pth
Saving shard 2 of 2 into alpaca-combined/L3-consolidated.01.pth
Saving tokenizer
Saving params.json into alpaca-combined/params.json
Loading ['L1-consolidated.00.pth', 'L2-consolidated.00.pth', 'L3-consolidated.00.pth'] ...
Saving the merged shard to alpaca-combined/consolidated.00.pth
Cleaning up...
Loading ['L1-consolidated.01.pth', 'L2-consolidated.01.pth', 'L3-consolidated.01.pth'] ...
Saving the merged shard to alpaca-combined/consolidated.01.pth
Cleaning up...
Done.
```

check the file:

```ini
tree alpaca-combined/
alpaca-combined/
├── consolidated.00.pth
├── consolidated.01.pth
├── params.json
├── special_tokens_map.json
├── tokenizer_config.json
└── tokenizer.model

```



### Convert

> build llama.cpp

```shell
# make sure gcc version is greated than 7
scl enable devtoolset-11 bash

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

make
```



```shell
cd llama.cpp 
mkdir zh-models
mv ../Chinese-LLaMA-Alpaca/alpaca-combined zh-models/13B

mv ./zh-models/13B/tokenizer.model ./zh-models/
ls ./zh-models/
```



```shell
 cd llama.cpp
 python convert.py zh-models/13B/
```

log as below:

```ini
[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')
[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')
[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')
[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')
Wrote zh-models/13B/ggml-model-f16.bin
```

ant check files:

```ini
du -ah zh-models/13B/
13G     zh-models/13B/consolidated.00.pth
4.0K    zh-models/13B/tokenizer_config.json
25G     zh-models/13B/ggml-model-f16.bin
13G     zh-models/13B/consolidated.01.pth
4.0K    zh-models/13B/special_tokens_map.json
4.0K    zh-models/13B/params.json
50G     zh-models/13B/
```



### Quantize FP16 model to 4-bit

```shell
cd llama.cpp 

./quantize ./zh-models/13B/ggml-model-f16.bin ./zh-models/13B/ggml-model-q4_K.bin q4_K
```

the log as below:

```ini
[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 
[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB
[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 
[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    55.37 MB | hist: 
[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 
[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB
llama_model_quantize_internal: model size  = 25177.25 MB
llama_model_quantize_internal: quant size  =  7622.08 MB
```



check files:

```ini
du -ah zh-models/13B/
13G     zh-models/13B/consolidated.00.pth
4.0K    zh-models/13B/tokenizer_config.json
25G     zh-models/13B/ggml-model-f16.bin
7.5G    zh-models/13B/ggml-model-q4_K.bin
13G     zh-models/13B/consolidated.01.pth
4.0K    zh-models/13B/special_tokens_map.json
4.0K    zh-models/13B/params.json
```





### Verify Model

```shell
./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "我国现行的税种有哪些" -n 128

./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "业蝶阀的主要参数有哪些？包括哪些标准和类型？请解释一下" -n 128

./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "给你一张学生表，包含学生编号，学生姓名，语文成绩，数学成绩，学年，请帮我写一个sql语句，查询2021年各科成绩分数总和最高的前5名学生" -n 2048

```



```ini
./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "工业蝶阀的主要参数有哪些" -n 128

./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "工业蝶阀的主要参数和主要标准有哪些？请给出具体参数名称和标准的说明。" -n 2048


./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "有没有“蝶阀，公称直径 65，公称压力 16 兆帕”的物料?" -n 2048

./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "参数特征符合 DN50\PN2.5MPa\J41H-25 规范的阀门有哪些?" -n 2048

./main -m ./zh-models/13B/ggml-model-q4_K.bin --color -p "符合参数公称直径65，公称压力1.6兆帕的阀门有哪些，请给出具体的阀门名称和型号?" -n 2048


```

