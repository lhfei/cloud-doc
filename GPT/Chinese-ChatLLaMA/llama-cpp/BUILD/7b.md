

### Check out source code

> Transformer Model

```shell
git lfs install
git clone https://huggingface.co/elinas/llama-7b-hf-transformers-4.29
```



> Lora Model

Take model `13b` as an example, please click [here](https://huggingface.co/ziqingyang)  for detailed model links.

```shell
git lfs install
git clone https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b
```



> project

```shell
git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca
git clone https://github.com/ggerganov/llama.cpp
```



```shell
cd Chinese-LLaMA-Alpaca

python ./scripts/merge_llama_with_chinese_lora_low_mem.py \
    --base_model '../llama-7b-hf-transformers-4.29' \
    --lora_model '../chinese-alpaca-lora-7b' \
    --output_type pth \
    --output_dir alpaca-combined
```

And the log like as below:

```ini
Base model: ../llama-7b-hf-transformers-4.29
LoRA model(s) ['../chinese-alpaca-lora-7b']:
Loading ../chinese-alpaca-lora-7b
Loading ckpt pytorch_model-00001-of-00002.bin
Merging...
Converting to pth format...
Saving shard 1 of 1 into alpaca-combined/L1-consolidated.00.pth
Loading ckpt pytorch_model-00002-of-00002.bin

Merging...
Converting to pth format...
Saving shard 1 of 1 into alpaca-combined/L2-consolidated.00.pth
Saving tokenizer
Saving params.json into alpaca-combined/params.json
Loading ['L1-consolidated.00.pth', 'L2-consolidated.00.pth'] ...
Saving the merged shard to alpaca-combined/consolidated.00.pth
Cleaning up...
Done.
```

check the file:

```ini
tree alpaca-combined/
alpaca-combined/
├── consolidated.00.pth
├── consolidated.01.pth
├── params.json
├── special_tokens_map.json
├── tokenizer_config.json
└── tokenizer.model

```



### Convert

> build llama.cpp

```shell
# make sure gcc version is greated than 7

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

make
```



```shell
cd llama.cpp 
mkdir zh-models
mv ../Chinese-LLaMA-Alpaca/alpaca-combined zh-models/7B

mv ./zh-models/7B/tokenizer.model ./zh-models/
ls ./zh-models/
```



```shell
 cd llama.cpp
  zh-models/7B/
```

log as below:

```ini
Loading model file zh-models/7B/consolidated.00.pth
Loading vocab file zh-models/tokenizer.model
Writing vocab...
......
[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')
[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')
[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')
[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
Wrote zh-models/7B/ggml-model-f16.bin
```

ant check files:

```ini
du -ah zh-models/7B/
13G     zh-models/7B/ggml-model-f16.bin
13G     zh-models/7B/consolidated.00.pth
4.0K    zh-models/7B/special_tokens_map.json
4.0K    zh-models/7B/tokenizer_config.json
4.0K    zh-models/7B/params.json
26G     zh-models/7B/
```



### Quantize FP16 model to 4-bit

```shell
cd llama.cpp 

./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_K.bin q4_K
```

the log as below:

```ini
[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 
[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB
[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 
[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    55.37 MB | hist: 
[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 
[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB
llama_model_quantize_internal: model size  = 25177.25 MB
llama_model_quantize_internal: quant size  =  7622.08 MB
```



check files:

```ini
du -ah zh-models/7B/
13G     zh-models/7B/consolidated.00.pth
4.0K    zh-models/7B/tokenizer_config.json
25G     zh-models/7B/ggml-model-f16.bin
7.5G    zh-models/7B/ggml-model-q4_K.bin
13G     zh-models/7B/consolidated.01.pth
4.0K    zh-models/7B/special_tokens_map.json
4.0K    zh-models/7B/params.json
```





### Verify Model

```shell
./main -m ./zh-models/7B/ggml-model-q4_K.bin --color -p "我国现行的税种有哪些" -n 128
```



```ini
./main -m ./zh-models/7B/ggml-model-q4_K.bin --color -p "工业蝶阀的主要参数有哪些" -n 128

./main -m ./zh-models/7B/ggml-model-q4_K.bin --color -p "有没有“蝶阀，公称直径 65，公称压力 16 兆帕”的物料?" -n 2048

```

