### 1 Required

####  1 GCC

```shell
sudo yum install -y centos-release-scl
sudo yum install -y devtoolset-11-gcc*
scl enable devtoolset-11 bash
```



### 2 Convert

#### 1 Llama-2-7b-hf

```shell
git lfs install
git clone git@hf.co:meta-llama/Llama-2-7b-hf
```



#### 2 chinese-alpaca-2-lora-7

```shell
git lfs install
git clone https://huggingface.co/ziqingyang/chinese-alpaca-2-lora-7b
```



#### 3 Chinese-LLaMA-Alpaca-2

```shell
git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca-2.git
```



#### 4 convert

```shell
cd Chinese-LLaMA-Alpaca-2
```

```shell
python scripts/merge_llama2_with_chinese_lora_low_mem.py \
    --base_model /export/app_workspaces/LLaMa/Llama-2-7b-hf \
    --lora_model /export/app_workspaces/LLaMa/chinese-alpaca-2-lora-7b \
    --output_type huggingface \
    --output_dir /export/app_workspaces/LLaMa/llama2-chinese-alpaca-2-lora-7b
```



### Convert

> build llama.cpp

```shell
# make sure gcc version is greated than 7

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

make
```



```shell
cd llama.cpp 
mkdir zh-models

cp -a ../llama2-chinese-alpaca-2-lora-7b zh-models/7B

mv ./zh-models/7B/tokenizer.model ./zh-models/
```



```shell
tree ./zh-models/
```

log as below:

```ini
zh-models/
├── 7B
│   ├── config.json
│   ├── generation_config.json
│   ├── pytorch_model-00001-of-00002.bin
│   ├── pytorch_model-00002-of-00002.bin
│   ├── pytorch_model.bin.index.json
│   ├── special_tokens_map.json
│   └── tokenizer_config.json
└── tokenizer.model
```

ant check files:

```ini
du -ah zh-models/7B/
4.0K    zh-models/7B/generation_config.json
4.0K    zh-models/7B/tokenizer_config.json
28K     zh-models/7B/pytorch_model.bin.index.json
4.0K    zh-models/7B/config.json
3.5G    zh-models/7B/pytorch_model-00002-of-00002.bin
4.0K    zh-models/7B/special_tokens_map.json
9.5G    zh-models/7B/pytorch_model-00001-of-00002.bin
13G     zh-models/7B/
```



### Quantize FP16 model to 4-bit

```shell
cd llama.cpp 

python convert.py zh-models/7B/
```

```ini
total 26G
drwxr-xr-x 2 root root 4.0K Aug  1 21:19 .
drwxr-xr-x 3 root root 4.0K Aug  1 21:16 ..
-rw-r--r-- 1 root root  608 Aug  1 20:26 config.json
-rw-r--r-- 1 root root  166 Aug  1 20:26 generation_config.json
-rw-r--r-- 1 root root  13G Aug  1 21:22 ggml-model-f16.bin
-rw-r--r-- 1 root root 9.5G Aug  1 20:25 pytorch_model-00001-of-00002.bin
-rw-r--r-- 1 root root 3.5G Aug  1 20:26 pytorch_model-00002-of-00002.bin
-rw-r--r-- 1 root root  27K Aug  1 20:26 pytorch_model.bin.index.json
-rw-r--r-- 1 root root  435 Aug  1 20:26 special_tokens_map.json
-rw-r--r-- 1 root root  766 Aug  1 20:26 tokenizer_config.json
```



```shell
./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin q4_0
```

the log as below:

```ini
75 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.036 0.023 0.019 
[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q4_0 .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 13217.02 MB
llama_model_quantize_internal: quant size  =  3773.70 MB
```



check files:

```ini
du -ah zh-models/7B/

4.0K    zh-models/7B/generation_config.json
4.0K    zh-models/7B/tokenizer_config.json
28K     zh-models/7B/pytorch_model.bin.index.json
4.0K    zh-models/7B/config.json
3.7G    zh-models/7B/ggml-model-q4_0.bin
3.5G    zh-models/7B/pytorch_model-00002-of-00002.bin
4.0K    zh-models/7B/special_tokens_map.json
13G     zh-models/7B/ggml-model-f16.bin
9.5G    zh-models/7B/pytorch_model-00001-of-00002.bin
```



### Verify Model

```shell
./main -m ./zh-models/7B/ggml-model-q4_0.bin --color -p "我国现行的税种有哪些" -n 256
```



```ini
./main -m ./zh-models/7B/ggml-model-q4_0.bin --color -p "工业蝶阀的主要参数有哪些" -n 256
```



